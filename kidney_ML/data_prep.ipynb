{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbf5af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc93d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Using cached openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Using cached et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Using cached openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Using cached et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0f2b5",
   "metadata": {},
   "source": [
    "Five files of biomarker data to be merged ans features selected here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44ee1e",
   "metadata": {},
   "source": [
    "Author: Neha Sindhu \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "268fa1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = os.path.expanduser(\"~/Documents/BINF/competition/Pre_Processed_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1423a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get .pkl files\n",
    "pkl_files = glob.glob(os.path.join(path, \"*.pkl\"))\n",
    "#Load them as df\n",
    "dataframes = [pd.read_pickle(f) for f in pkl_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61821dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dict keyed by filename\n",
    "df_dict = {os.path.basename(f).replace(\".pkl\", \"\"): pd.read_pickle(f) for f in pkl_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d633a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1326\n"
     ]
    }
   ],
   "source": [
    "#concatenate them into one huge dataframe\n",
    "big_df = pd.concat(\n",
    "    [df.assign(source=name) for name, df in df_dict.items()],\n",
    "    ignore_index=True,\n",
    "    sort=False\n",
    ")\n",
    "\n",
    "# sanity check: should be 25 if 5×5\n",
    "print(len(big_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ee14280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant ID_clinical_df1</th>\n",
       "      <th>Tissue Source_df1</th>\n",
       "      <th>Protocol_df1</th>\n",
       "      <th>Sample Type_clinical_df1</th>\n",
       "      <th>Enrollment Category_df1</th>\n",
       "      <th>Primary Adjudicated Category_df1</th>\n",
       "      <th>Sex_df1</th>\n",
       "      <th>Age (Years) (Binned)_df1</th>\n",
       "      <th>Race_df1</th>\n",
       "      <th>KDIGO Stage_df1</th>\n",
       "      <th>...</th>\n",
       "      <th>RACE_df2</th>\n",
       "      <th>SAMPLE_AMOUNT_df2</th>\n",
       "      <th>SAMPLE_AMOUNT_UNITS_df2</th>\n",
       "      <th>SAMPLE_BOX_LOCATION_df2</th>\n",
       "      <th>VOLUME_EXTRACTED</th>\n",
       "      <th>Participant ID_OP-UHPLC MS__df2</th>\n",
       "      <th>Sample Type_OP-UHPLC MS__df2</th>\n",
       "      <th>Study type_df2</th>\n",
       "      <th>Biopsy Time Interval Days Post Consent</th>\n",
       "      <th>Sample Collection Interval Days Post Consent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27-10039</td>\n",
       "      <td>KPMP Recruitment Site</td>\n",
       "      <td>KPMP Main Protocol</td>\n",
       "      <td>Percutaneous Needle Biopsy</td>\n",
       "      <td>CKD</td>\n",
       "      <td>Diabetic Kidney Disease</td>\n",
       "      <td>Female</td>\n",
       "      <td>60-69 Years</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27-10039</td>\n",
       "      <td>KPMP Recruitment Site</td>\n",
       "      <td>KPMP Main Protocol</td>\n",
       "      <td>Percutaneous Needle Biopsy</td>\n",
       "      <td>CKD</td>\n",
       "      <td>Diabetic Kidney Disease</td>\n",
       "      <td>Female</td>\n",
       "      <td>60-69 Years</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27-10041</td>\n",
       "      <td>KPMP Recruitment Site</td>\n",
       "      <td>KPMP Main Protocol</td>\n",
       "      <td>Percutaneous Needle Biopsy</td>\n",
       "      <td>CKD</td>\n",
       "      <td>Diabetic Kidney Disease</td>\n",
       "      <td>Female</td>\n",
       "      <td>60-69 Years</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27-10041</td>\n",
       "      <td>KPMP Recruitment Site</td>\n",
       "      <td>KPMP Main Protocol</td>\n",
       "      <td>Percutaneous Needle Biopsy</td>\n",
       "      <td>CKD</td>\n",
       "      <td>Diabetic Kidney Disease</td>\n",
       "      <td>Female</td>\n",
       "      <td>60-69 Years</td>\n",
       "      <td>Asian</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>27-10042</td>\n",
       "      <td>KPMP Recruitment Site</td>\n",
       "      <td>KPMP Main Protocol</td>\n",
       "      <td>Percutaneous Needle Biopsy</td>\n",
       "      <td>CKD</td>\n",
       "      <td>Diabetic Kidney Disease</td>\n",
       "      <td>Female</td>\n",
       "      <td>50-59 Years</td>\n",
       "      <td>White</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 236 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant ID_clinical_df1      Tissue Source_df1        Protocol_df1  \\\n",
       "0                    27-10039  KPMP Recruitment Site  KPMP Main Protocol   \n",
       "1                    27-10039  KPMP Recruitment Site  KPMP Main Protocol   \n",
       "2                    27-10041  KPMP Recruitment Site  KPMP Main Protocol   \n",
       "3                    27-10041  KPMP Recruitment Site  KPMP Main Protocol   \n",
       "4                    27-10042  KPMP Recruitment Site  KPMP Main Protocol   \n",
       "\n",
       "     Sample Type_clinical_df1 Enrollment Category_df1  \\\n",
       "0  Percutaneous Needle Biopsy                     CKD   \n",
       "1  Percutaneous Needle Biopsy                     CKD   \n",
       "2  Percutaneous Needle Biopsy                     CKD   \n",
       "3  Percutaneous Needle Biopsy                     CKD   \n",
       "4  Percutaneous Needle Biopsy                     CKD   \n",
       "\n",
       "  Primary Adjudicated Category_df1 Sex_df1 Age (Years) (Binned)_df1 Race_df1  \\\n",
       "0          Diabetic Kidney Disease  Female              60-69 Years    White   \n",
       "1          Diabetic Kidney Disease  Female              60-69 Years    White   \n",
       "2          Diabetic Kidney Disease  Female              60-69 Years    Asian   \n",
       "3          Diabetic Kidney Disease  Female              60-69 Years    Asian   \n",
       "4          Diabetic Kidney Disease  Female              50-59 Years    White   \n",
       "\n",
       "   KDIGO Stage_df1  ...  RACE_df2 SAMPLE_AMOUNT_df2 SAMPLE_AMOUNT_UNITS_df2  \\\n",
       "0              NaN  ...       NaN               NaN                     NaN   \n",
       "1              NaN  ...       NaN               NaN                     NaN   \n",
       "2              NaN  ...       NaN               NaN                     NaN   \n",
       "3              NaN  ...       NaN               NaN                     NaN   \n",
       "4              NaN  ...       NaN               NaN                     NaN   \n",
       "\n",
       "  SAMPLE_BOX_LOCATION_df2 VOLUME_EXTRACTED Participant ID_OP-UHPLC MS__df2  \\\n",
       "0                     NaN              NaN                             NaN   \n",
       "1                     NaN              NaN                             NaN   \n",
       "2                     NaN              NaN                             NaN   \n",
       "3                     NaN              NaN                             NaN   \n",
       "4                     NaN              NaN                             NaN   \n",
       "\n",
       "  Sample Type_OP-UHPLC MS__df2 Study type_df2  \\\n",
       "0                          NaN            NaN   \n",
       "1                          NaN            NaN   \n",
       "2                          NaN            NaN   \n",
       "3                          NaN            NaN   \n",
       "4                          NaN            NaN   \n",
       "\n",
       "  Biopsy Time Interval Days Post Consent  \\\n",
       "0                                    NaN   \n",
       "1                                    NaN   \n",
       "2                                    NaN   \n",
       "3                                    NaN   \n",
       "4                                    NaN   \n",
       "\n",
       "  Sample Collection Interval Days Post Consent  \n",
       "0                                          NaN  \n",
       "1                                          NaN  \n",
       "2                                          NaN  \n",
       "3                                          NaN  \n",
       "4                                          NaN  \n",
       "\n",
       "[5 rows x 236 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baa45b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#save big_df and the column names\n",
    "\n",
    "# Save column names\n",
    "# UTF-8 text file for column names\n",
    "with open(os.path.join(path, \"big_df_columns.txt\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "    for col in big_df.columns:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "#Save full dataframe\n",
    "big_df.to_excel(os.path.join(path, \"big_df.xlsx\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e21e7fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "#find and delete the columns where all rows had the same entry. \n",
    "# write those column names into a txt file\n",
    "\n",
    "# find columns with the same value in all rows\n",
    "constant_cols = [col for col in big_df.columns if big_df[col].nunique() == 1]\n",
    "print(len(constant_cols))\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "# save their names\n",
    "\n",
    "with open(os.path.join(path, \"constant_columns.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    for col in constant_cols:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "\n",
    "# drop those columns from the dataframe\n",
    "big_df = big_df.drop(columns=constant_cols)\n",
    "\n",
    "#save the cleaned dataframe\n",
    "big_df.to_excel(os.path.join(path, \"big_df1.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ec15be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete column containing the types of participant visit\n",
    "cols_to_drop = [col for col in big_df.columns if col.startswith(\"Participant Visit\")]\n",
    "big_df = big_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab0f1131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1326, Columns: 191\n"
     ]
    }
   ],
   "source": [
    "#view the df size\n",
    "\n",
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7bb370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete every column starting with 'Participant' except the column Participant_ID_norm\n",
    "\n",
    "if big_df[\"Participant_ID_norm\"].isna().sum() == 0:\n",
    "    cols_to_drop = [col for col in big_df.columns\n",
    "                    if col.startswith(\"Participant\") and col != \"Participant_ID_norm\"]\n",
    "    big_df = big_df.drop(columns=cols_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "944172e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1326, Columns: 181\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94a8d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [col for col in big_df.columns if \"Sample\" in col or \"SAMPLE\" in col]\n",
    "big_df = big_df.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c04756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 1326, Columns: 162\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fcc1234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#are columns Biopsy Time Interval Days Post Consent_df1 and Biopsy Time Interval Days Post Consent_df2 identical?\n",
    "(big_df[\"Biopsy Time Interval Days Post Consent_df1\"]\n",
    " .equals(big_df[\"Biopsy Time Interval Days Post Consent_df2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41b8d178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\neha\\AppData\\Local\\Temp\\ipykernel_3648\\1748210524.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  big_df[\"Days_Post_Biopsy\"] = big_df[\"Biopsy Time Interval Days Post Consent_df1\"].combine_first(\n"
     ]
    }
   ],
   "source": [
    "#big_df = big_df.drop(columns=[\"Biopsy Time Interval Days Post Consent\"])\n",
    "big_df[\"Days_Post_Biopsy\"] = big_df[\"Biopsy Time Interval Days Post Consent_df1\"].combine_first(\n",
    "    big_df[\"Biopsy Time Interval Days Post Consent_df2\"]\n",
    ")\n",
    "big_df = big_df.drop(columns=[\n",
    "    \"Biopsy Time Interval Days Post Consent_df1\",\n",
    "    \"Biopsy Time Interval Days Post Consent_df2\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "608e9a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   NaN\n",
       "1   NaN\n",
       "2   NaN\n",
       "3   NaN\n",
       "4   NaN\n",
       "Name: Biopsy Time Interval Days Post Consent, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_df[\"Biopsy Time Interval Days Post Consent\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922e5496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.False_"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#are there rows where there are non NaN values in both columns Days_Post_Biosy and Biopsy Time Interval Days Post Consent\n",
    "mask = big_df[\"Days_Post_Biopsy\"].notna() & big_df[\"Biopsy Time Interval Days Post Consent\"].notna()\n",
    "mask.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7067b9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the columns Days_Post_Biosy and Biopsy Time Interval Days Post Consent. the entry of the resulting columns shouls be whatever the value was on the non-nan values in the original columns.\n",
    "#  The resulting column are named Days_after_biopsy\n",
    "big_df[\"Days_after_biopsy\"] = big_df[\"Days_Post_Biopsy\"].combine_first(\n",
    "    big_df[\"Biopsy Time Interval Days Post Consent\"]\n",
    ")\n",
    "big_df = big_df.drop(columns=[\n",
    "    \"Days_Post_Biopsy\",\n",
    "    \"Biopsy Time Interval Days Post Consent\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b6d51e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(118)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "big_df[\"Days_after_biopsy\"].isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9bcbe761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all rows whose days post biopsy is NaN, replace it with -1 (to replace later)\n",
    "big_df[\"Days_after_biopsy\"] = big_df[\"Days_after_biopsy\"].fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b1072c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row has no DAB\n",
      "row has no DAB\n",
      "row has no DAB\n"
     ]
    }
   ],
   "source": [
    "#for every set of rows with the same Participant_ID_norm but different Days_after_Biopsy, \n",
    "# delete all rows except the one whose Days_after_Biopsy is the smallest positive integer.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# pick the correct column name\n",
    "dab_col = \"Days_after_biopsy\" if \"Days_after_biopsy\" in big_df.columns else \"Days_after_Biopsy\"\n",
    "\n",
    "# ensure numeric\n",
    "big_df[dab_col] = pd.to_numeric(big_df[dab_col], errors=\"coerce\")\n",
    "\n",
    "# positive integers only\n",
    "pos_int = (big_df[dab_col] > 0) & ((big_df[dab_col] % 1) == 0)\n",
    "\n",
    "# smallest positive integer per participant\n",
    "idx_keep_pos = (big_df[pos_int]\n",
    "                .groupby(\"Participant_ID_norm\")[dab_col]\n",
    "                .idxmin())\n",
    "\n",
    "# participants with no positive DAB\n",
    "all_pids = big_df[\"Participant_ID_norm\"].unique()\n",
    "pids_with_pos = big_df.loc[pos_int, \"Participant_ID_norm\"].unique()\n",
    "pids_no_pos = set(all_pids) - set(pids_with_pos)\n",
    "\n",
    "idx_keep_no = []\n",
    "for pid in pids_no_pos:\n",
    "    grp = big_df[big_df[\"Participant_ID_norm\"] == pid]\n",
    "    cand = grp.index[grp[dab_col] == -1]\n",
    "    if len(cand):\n",
    "        idx_keep_no.append(cand[0])\n",
    "        print(\"row has no DAB\")  # one print per such participant\n",
    "    else:\n",
    "        # fallback: keep the first row if -1 is missing (should be rare)\n",
    "        idx_keep_no.append(grp.index[0])\n",
    "        print(\"row has no DAB\")\n",
    "\n",
    "keep_idx = pd.Index(idx_keep_pos.tolist() + idx_keep_no)\n",
    "big_df = big_df.loc[keep_idx].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee910677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 207, Columns: 160\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6d81f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For all rows whose days post biopsy is -1, replace it with 0\n",
    "big_df[\"Days_after_biopsy\"] = big_df[\"Days_after_biopsy\"].replace(-1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bdc7034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check if Participant_ID_norm is unique\n",
    "big_df[\"Participant_ID_norm\"].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838e3c7",
   "metadata": {},
   "source": [
    "Now we have 207 patients' data from their earliest possible time point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abadb2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save big_df and the column names\n",
    "\n",
    "# Save column names\n",
    "# UTF-8 text file for column names\n",
    "with open(os.path.join(path, \"uniquePatient_columns.txt\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "    for col in big_df.columns:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "#Save full dataframe\n",
    "big_df.to_excel(os.path.join(path, \"uniquePatient.xlsx\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f6191b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all columns without at least 10 non-NaN values\n",
    "\n",
    "big_df = big_df.dropna(axis=1, thresh=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed71920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all columns whose non-NA values are all identical\n",
    "big_df = big_df.drop(columns=[col for col in big_df.columns if big_df[col].nunique(dropna=True) == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6aac61e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 207, Columns: 66\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "11352287",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save big_df and the column names\n",
    "\n",
    "# Save column names\n",
    "# UTF-8 text file for column names\n",
    "with open(os.path.join(path, \"uniquePatient_columns2.txt\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "    for col in big_df.columns:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "#Save full dataframe\n",
    "big_df.to_excel(os.path.join(path, \"uniquePatient2.xlsx\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b9292d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged into 'Primary Adjudicated Category_df1': Primary Adjudicated Category_df2\n",
      "merged into 'Sex_df1': Sex_df2\n",
      "merged into 'Age (Years) (Binned)_df1': Age (Years) (Binned)_df2\n",
      "merged into 'Race_df1': Race_df2\n",
      "merged into 'Baseline eGFR (ml/min/1.73m2)_df1': Baseline eGFR (ml/min/1.73m2)_df2\n",
      "merged into 'Baseline eGFR (ml/min/1.73m2) (Binned)_df1': Baseline eGFR (ml/min/1.73m2) (Binned)_df2\n",
      "merged into 'Proteinuria (mg) (Binned)_df1': Proteinuria (mg) (Binned)_df2\n",
      "merged into 'A1c (%) (Binned)_df1': A1c (%) (Binned)_df2\n",
      "merged into 'Albuminuria (mg) (Binned)_df1': Albuminuria (mg) (Binned)_df2\n",
      "merged into 'Diabetes History_df1': Diabetes History_df2\n",
      "merged into 'Diabetes Duration (Years)_df1': Diabetes Duration (Years)_df2\n",
      "merged into 'Hypertension History_df1': Hypertension History_df2\n",
      "merged into 'Hypertension Duration (Years)_df1': Hypertension Duration (Years)_df2\n",
      "merged into 'On RAAS Blockade_df1': On RAAS Blockade_df2\n"
     ]
    }
   ],
   "source": [
    "#check if there are pairs of column that have exactly the same entries. \n",
    "# If yes, merge them, and rename them with the name of the first of the column\n",
    "\n",
    "# identify sets of columns with identical entries (NaNs in the same rows count as identical)\n",
    "col_hash = {\n",
    "    col: pd.util.hash_pandas_object(big_df[col], index=False).values.tobytes()\n",
    "    for col in big_df.columns\n",
    "}\n",
    "\n",
    "groups = {}\n",
    "for col, h in col_hash.items():\n",
    "    groups.setdefault(h, []).append(col)\n",
    "\n",
    "# columns to drop (keep the first name in each identical set)\n",
    "to_drop = []\n",
    "merged_pairs = []\n",
    "for cols in groups.values():\n",
    "    if len(cols) > 1:\n",
    "        merged_pairs.append((cols[0], cols[1:]))  # kept, dropped\n",
    "        to_drop.extend(cols[1:])\n",
    "\n",
    "# drop duplicate-identical columns\n",
    "if to_drop:\n",
    "    big_df = big_df.drop(columns=to_drop)\n",
    "\n",
    "# optional: view what was merged\n",
    "for kept, dropped in merged_pairs:\n",
    "    print(f\"merged into '{kept}': {', '.join(dropped)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c35d98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 207, Columns: 52\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "029a57bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save big_df and the column names\n",
    "\n",
    "# Save column names\n",
    "# UTF-8 text file for column names\n",
    "with open(os.path.join(path, \"uniquePatient_columns3.txt\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "    for col in big_df.columns:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "#Save full dataframe\n",
    "big_df.to_excel(os.path.join(path, \"uniquePatient3.xlsx\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "938d33a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_total': 207, 'n_excluded': 0, 'pct_excluded': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heme index</th>\n",
       "      <th>Lipemic index</th>\n",
       "      <th>Icteric index</th>\n",
       "      <th>exclude_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Heme index, Lipemic index, Icteric index, exclude_reason]\n",
       "Index: []"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The heme, lipemic, and icteric indices are sample quality control metrics, not biological variables.\n",
    "#remove rows with too high values for these: per-site z-scores, applies absolute cutoffs.\n",
    "#then remove the columns\n",
    "\n",
    "# Define thresholds\n",
    "HI_COL = \"Heme index\"      \n",
    "LI_COL = \"Lipemic index\"   \n",
    "II_COL = \"Icteric index\"   \n",
    "\n",
    "# ---- ABSOLUTE EXCLUSION THRESHOLDS ----\n",
    "HI_MAX = 100.0   # > 100 → exclude (gross hemolysis)\n",
    "LI_MAX = 200.0   # > 200 → exclude (gross lipemia)\n",
    "II_MAX = 10.0    # > 10  → exclude (gross icterus)\n",
    "\n",
    "# ---- APPLY QC FILTER ----\n",
    "df = big_df.copy()\n",
    "\n",
    "# Ensure numeric\n",
    "for c in [HI_COL, LI_COL, II_COL]:\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "# Identify threshold failures\n",
    "df[\"fail_heme\"]   = df[HI_COL] > HI_MAX\n",
    "df[\"fail_lipemic\"] = df[LI_COL] > LI_MAX\n",
    "df[\"fail_icteric\"] = df[II_COL] > II_MAX\n",
    "\n",
    "# Combine\n",
    "df[\"exclude\"] = df[[\"fail_heme\", \"fail_lipemic\", \"fail_icteric\"]].any(axis=1)\n",
    "\n",
    "# Record reason(s)\n",
    "def get_reason(row):\n",
    "    reasons = []\n",
    "    if row[\"fail_heme\"]:\n",
    "        reasons.append(f\"{HI_COL}>{HI_MAX}\")\n",
    "    if row[\"fail_lipemic\"]:\n",
    "        reasons.append(f\"{LI_COL}>{LI_MAX}\")\n",
    "    if row[\"fail_icteric\"]:\n",
    "        reasons.append(f\"{II_COL}>{II_MAX}\")\n",
    "    return \", \".join(reasons)\n",
    "\n",
    "df[\"exclude_reason\"] = df.apply(get_reason, axis=1)\n",
    "\n",
    "# Split outputs\n",
    "filtered_df = df.loc[~df[\"exclude\"]].copy()\n",
    "exclusions_df = df.loc[df[\"exclude\"], \n",
    "                       [HI_COL, LI_COL, II_COL, \"exclude_reason\"]].copy()\n",
    "\n",
    "# QC summary\n",
    "qc_report = {\n",
    "    \"n_total\": int(len(df)),\n",
    "    \"n_excluded\": int(df[\"exclude\"].sum()),\n",
    "    \"pct_excluded\": float(100 * df[\"exclude\"].mean()),\n",
    "}\n",
    "\n",
    "print(qc_report)\n",
    "filtered_df.head()\n",
    "exclusions_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09b995",
   "metadata": {},
   "source": [
    "All indices are within threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "43aaaed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the columns of the three QC indices\n",
    "\n",
    "big_df = big_df.drop(columns=[\"Heme index\", \"Lipemic index\", \"Icteric index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "731c4e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 207, Columns: 49\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "567ead2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that are relevant to the protocol, but not meaningful model inputs\n",
    "\n",
    "big_df = big_df.drop(columns=[\"Total volume (L)\", \"Total hours\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b5a7e6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that should not be used\n",
    "\n",
    "##source (dataset origin: confounding risk)\n",
    "##Primary Adjudicated Category_df1 (too close to outcome, risk of leakage)\n",
    "##Days_after_biopsy (time index, not intrinsic biology)\n",
    "\n",
    "big_df = big_df.drop(columns=[\"source\", \"Primary Adjudicated Category_df1\",\"Days_after_biopsy\" ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1cea9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 207, Columns: 44\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "009c0e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save big_df and the column names\n",
    "\n",
    "# Save column names\n",
    "# UTF-8 text file for column names\n",
    "with open(os.path.join(path, \"uniquePatient_columns4.txt\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "    for col in big_df.columns:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "#Save full dataframe\n",
    "big_df.to_excel(os.path.join(path, \"uniquePatient4.xlsx\"), index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a283b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. If the same variable is given as absolute value in one column, and binned in another, then the binned column is removed\n",
    "#merge the columns that indicate the same things, but have different column names\n",
    "big_df[\"Creatinine_Index_mgkgday\"] = big_df[\"Creatinine Index (mg/kg/day)\"].combine_first(\n",
    "    big_df[\"Creatinine Index(mg/kg/day)\"]\n",
    ")\n",
    "big_df = big_df.drop(columns=[\"Creatinine Index (mg/kg/day)\", \"Creatinine Index(mg/kg/day)\", \"Baseline eGFR (ml/min/1.73m2) (Binned)_df1\"])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcb3b5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save big_df and the column names\n",
    "\n",
    "# Save column names\n",
    "# UTF-8 text file for column names\n",
    "with open(os.path.join(path, \"uniquePatient_columns5.txt\"), \"w\", encoding=\"utf-8\", newline=\"\\n\") as f:\n",
    "    for col in big_df.columns:\n",
    "        f.write(f\"{col}\\n\")\n",
    "\n",
    "#Save full dataframe\n",
    "big_df.to_excel(os.path.join(path, \"uniquePatient5.xlsx\"), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85988a9",
   "metadata": {},
   "source": [
    "On the downloaded data: \n",
    "1. changed column names to be more readable (removed _df etc) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3553bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 207, Columns: 42\n"
     ]
    }
   ],
   "source": [
    "print(f\"Rows: {big_df.shape[0]}, Columns: {big_df.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
